{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install pymilvus[milvus_lite]\n",
        "%pip install transformers\n",
        "%pip install datasets\n",
        "%pip install sentence-transformers\n",
        "%pip install ragas\n",
        "%pip install evaluate"
      ],
      "metadata": {
        "id": "hrtZzonE6wXl"
      },
      "id": "hrtZzonE6wXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fc17bf",
      "metadata": {
        "id": "49fc17bf"
      },
      "outputs": [],
      "source": [
        "# Load required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers, torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"llm\": \"google/flan-t5-base\",\n",
        "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "    \"embedding_dim\": \"384\",\n",
        "    \"prompting_style\": \"persona\",\n",
        "    \"rag_collection_name\": \"rag_mini\"\n",
        "}"
      ],
      "metadata": {
        "id": "J--3G9EvMSdL"
      },
      "id": "J--3G9EvMSdL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are a helpful assistant. Answer the question  accurately and concisely.\"\"\""
      ],
      "metadata": {
        "id": "VwsIjObJPTzM"
      },
      "id": "VwsIjObJPTzM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "queries = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\")\n",
        "\n",
        "print(f\"Queries dataset shape: {queries.shape}\")\n",
        "print(f\"Queries columns: {queries.columns.tolist()}\")\n"
      ],
      "metadata": {
        "id": "IMu2d1-iUp8H"
      },
      "id": "IMu2d1-iUp8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the queries dataset\n",
        "queries = queries.dropna(subset=['question', 'answer'])\n",
        "queries = queries[queries['question'].str.strip() != '']\n",
        "queries = queries[queries['answer'].str.strip() != '']\n",
        "\n",
        "print(f\"Cleaned queries shape: {queries.shape}\")\n",
        "print(\"\\nSample queries:\")\n",
        "print(queries.head())"
      ],
      "metadata": {
        "id": "4VTBdU5lUrlt"
      },
      "id": "4VTBdU5lUrlt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLM\n",
        "model_name = config['llm']\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on device: {device}\")\n"
      ],
      "metadata": {
        "id": "m_6ESTIfUuBU"
      },
      "id": "m_6ESTIfUuBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question, top_k=1, model=model, tokenizer=tokenizer):\n",
        "\n",
        "    # Create prompt\n",
        "\n",
        "    prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=150,\n",
        "            num_beams=4,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_answer\n",
        "\n",
        "# Test with different strategies and parameters\n",
        "strategies = {\n",
        "    \"NO RAG\": 1,\n",
        "}\n",
        "\n",
        "results = {}"
      ],
      "metadata": {
        "id": "CtHF0V9TZM61"
      },
      "id": "CtHF0V9TZM61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate responses for different strategies\n",
        "\n",
        "for strategy_name, top_k in strategies.items():\n",
        "    print(f\"\\nGenerating responses with {strategy_name} strategy...\")\n",
        "\n",
        "    strategy_results = {\n",
        "        'questions': [],\n",
        "        'generated_answers': [],\n",
        "        'ground_truth_answers': [],\n",
        "    }\n",
        "\n",
        "    for idx, row in tqdm(queries.iterrows(), total=len(queries), desc=f\"Processing {strategy_name}\"):\n",
        "        question = row['question']\n",
        "        ground_truth = row['answer']\n",
        "\n",
        "        generated_answer = get_response(question, top_k=top_k)\n",
        "\n",
        "        strategy_results['questions'].append(question)\n",
        "        strategy_results['generated_answers'].append(generated_answer)\n",
        "        strategy_results['ground_truth_answers'].append(ground_truth)\n",
        "\n",
        "    results[strategy_name] = strategy_results\n",
        "\n",
        "print(\"\\nResponse generation completed!\")\n"
      ],
      "metadata": {
        "id": "pnXmkrboUt3j"
      },
      "id": "pnXmkrboUt3j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s):\n",
        "    # Normalize answer for comparison\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    # Calculate exact match score\n",
        "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def f1_score_qa(prediction, ground_truth):\n",
        "    # Calculate F1 score for QA\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "print(\"QA METRICS EVALUATION\")\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for strategy_name, strategy_data in results.items():\n",
        "    print(f\"\\nEvaluating {strategy_name} strategy:\")\n",
        "\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for pred, truth in zip(strategy_data['generated_answers'], strategy_data['ground_truth_answers']):\n",
        "        em_score = exact_match_score(pred, truth)\n",
        "        f1_score = f1_score_qa(pred, truth)\n",
        "\n",
        "        em_scores.append(em_score)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "    avg_em = np.mean(em_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    evaluation_results[strategy_name] = {\n",
        "        'exact_match': avg_em,\n",
        "        'f1_score': avg_f1,\n",
        "        'em_scores': em_scores,\n",
        "        'f1_scores': f1_scores\n",
        "    }\n",
        "\n",
        "    print(f\"Exact Match: {avg_em:.4f}\")\n",
        "    print(f\"F1 Score: {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "Y0Zr2YQOU7uM"
      },
      "id": "Y0Zr2YQOU7uM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results comparison\n",
        "print(\"STRATEGY COMPARISON\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Strategy': list(evaluation_results.keys()),\n",
        "    'Exact Match': [evaluation_results[k]['exact_match'] for k in evaluation_results.keys()],\n",
        "    'F1 Score': [evaluation_results[k]['f1_score'] for k in evaluation_results.keys()]\n",
        "})\n",
        "\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "c8m1eKuTU7ri"
      },
      "id": "c8m1eKuTU7ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_data = {\n",
        "    'metadata': {\n",
        "        'date': datetime.now().isoformat(),\n",
        "        'dataset': 'RAG Mini Wikipedia',\n",
        "        'total_queries': len(queries),\n",
        "        'embedding_model': 'all-mpnet-base-v2',\n",
        "        'llm_model': 'google/flan-t5-base',\n",
        "        'embedding_dim': config['embedding_dim'],\n",
        "        'prompting_style': config['prompting_style'],\n",
        "    },\n",
        "    'strategy_comparison': comparison_df.to_dict('records'),\n",
        "    'detailed_results': {}\n",
        "}\n",
        "\n",
        "# Add results for each strategy\n",
        "for strategy_name, strategy_data in evaluation_results.items():\n",
        "    results_data['detailed_results'][strategy_name] = {\n",
        "        'exact_match': float(strategy_data['exact_match']),\n",
        "        'f1_score': float(strategy_data['f1_score']),\n",
        "        'num_samples': len(strategy_data['em_scores']),\n",
        "        'statistics': {\n",
        "            'em_std': float(np.std(strategy_data['em_scores'])),\n",
        "            'f1_std': float(np.std(strategy_data['f1_scores'])),\n",
        "            'em_min': float(np.min(strategy_data['em_scores'])),\n",
        "            'em_max': float(np.max(strategy_data['em_scores'])),\n",
        "            'f1_min': float(np.min(strategy_data['f1_scores'])),\n",
        "            'f1_max': float(np.max(strategy_data['f1_scores']))\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# Save to JSON\n",
        "output_filename = f'results_noRAG_{config['embedding_dim']}_{config['prompting_style']}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(results_data, indent=2, fp=f)\n",
        "\n",
        "print(f\"Results saved to {output_filename}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nResults Summary:\")\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "H1QM5YHDlwIV"
      },
      "id": "H1QM5YHDlwIV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}