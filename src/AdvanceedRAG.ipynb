{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install pymilvus[milvus_lite]\n",
        "%pip install transformers\n",
        "%pip install datasets\n",
        "%pip install sentence-transformers\n",
        "%pip install ragas\n",
        "%pip install evaluate"
      ],
      "metadata": {
        "id": "hrtZzonE6wXl"
      },
      "id": "hrtZzonE6wXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fc17bf",
      "metadata": {
        "id": "49fc17bf"
      },
      "outputs": [],
      "source": [
        "# Load required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers, torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"llm\": \"google/flan-t5-large\",\n",
        "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "    \"embedding_dim\": \"384\",\n",
        "    \"prompting_style\": \"advanced\",\n",
        "    \"rag_collection_name\": \"rag_mini\"\n",
        "}"
      ],
      "metadata": {
        "id": "J--3G9EvMSdL"
      },
      "id": "J--3G9EvMSdL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "275ab245",
      "metadata": {
        "id": "275ab245"
      },
      "source": [
        "# Read Passages from the Datasets and Drop rows if they are NA or empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0e11a5",
      "metadata": {
        "id": "ff0e11a5"
      },
      "outputs": [],
      "source": [
        "passages = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\")\n",
        "\n",
        "print(f\"Original dataset shape: {passages.shape}\")\n",
        "# Clean data\n",
        "passages = passages.dropna(subset=['passage'])\n",
        "passages = passages[passages['passage'].str.strip() != '']\n",
        "print(f\"Cleaned dataset shape: {passages.shape}\")\n",
        "passages.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset EDA\n"
      ],
      "metadata": {
        "id": "3rhIUmFtv39p"
      },
      "id": "3rhIUmFtv39p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze passage lengths before indexing\n",
        "\n",
        "print(\"Dataseet EDA\")\n",
        "print(f\"Total passages: {len(passages)}\")\n",
        "print(f\"Dataset columns: {passages.columns.tolist()}\")\n",
        "\n",
        "# Calculate passage lengths\n",
        "passages['passage_length'] = passages['passage'].str.len()\n",
        "passages['word_count'] = passages['passage'].str.split().str.len()\n",
        "\n",
        "print(f\"\\nPassage Length Statistics - Characters:\")\n",
        "print(f\"Min length: {passages['passage_length'].min()}\")\n",
        "print(f\"Max length: {passages['passage_length'].max()}\")\n",
        "print(f\"Mean length: {passages['passage_length'].mean():.2f}\")\n",
        "print(f\"Median length: {passages['passage_length'].median():.2f}\")\n",
        "print(f\"Std length: {passages['passage_length'].std():.2f}\")\n",
        "\n",
        "print(f\"\\nWord Count Statistics:\")\n",
        "print(f\"Min words: {passages['word_count'].min()}\")\n",
        "print(f\"Max words: {passages['word_count'].max()}\")\n",
        "print(f\"Mean words: {passages['word_count'].mean():.2f}\")\n",
        "print(f\"Median words: {passages['word_count'].median():.2f}\")\n",
        "\n",
        "# Distribution analysis\n",
        "print(f\"\\nPassage Length Distribution:\")\n",
        "print(passages['passage_length'].describe())"
      ],
      "metadata": {
        "id": "hyl3czQs7wAw"
      },
      "id": "hyl3czQs7wAw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distributions\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.hist(passages['passage_length'], bins=50, alpha=0.7, edgecolor='black')\n",
        "ax1.set_xlabel('Passage Length (characters)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Distribution of Passage Lengths')\n",
        "\n",
        "ax2.hist(passages['word_count'], bins=50, alpha=0.7, edgecolor='black')\n",
        "ax2.set_xlabel('Word Count')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Distribution of Word Counts')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2E2Ten076dp5"
      },
      "id": "2E2Ten076dp5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample passages\n",
        "print(f\"\\nSample passages:\")\n",
        "print(\"Short passage example:\")\n",
        "short_passage = passages[passages['passage_length'] < 100]['passage'].iloc[0] if len(passages[passages['passage_length'] < 100]) > 0 else \"No short passages found\"\n",
        "print(f\"Length: {len(short_passage)} chars\")\n",
        "print(f\"Content: {short_passage[:200]}...\")\n",
        "\n",
        "print(\"\\nMedium passage example:\")\n",
        "medium_mask = (passages['passage_length'] >= 200) & (passages['passage_length'] <= 400)\n",
        "medium_passage = passages[medium_mask]['passage'].iloc[0] if len(passages[medium_mask]) > 0 else \"No medium passages found\"\n",
        "print(f\"Length: {len(medium_passage)} chars\")\n",
        "print(f\"Content: {medium_passage[:200]}...\")\n",
        "\n",
        "print(\"\\nLong passage example:\")\n",
        "long_passage = passages[passages['passage_length'] > 600]['passage'].iloc[0] if len(passages[passages['passage_length'] > 600]) > 0 else \"No long passages found\"\n",
        "print(f\"Length: {len(long_passage)} chars\")\n",
        "print(f\"Content: {long_passage[:200]}...\")"
      ],
      "metadata": {
        "id": "qhMe2i_xF-R1"
      },
      "id": "qhMe2i_xF-R1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a6b32f3",
      "metadata": {
        "id": "6a6b32f3"
      },
      "source": [
        "# Tokenize Text and Generate Embeddings using Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(config['embedding_model'])"
      ],
      "metadata": {
        "id": "zuuOEYDbIbCG"
      },
      "id": "zuuOEYDbIbCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Text\n",
        "batch_size = 64\n",
        "embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding passages\"):\n",
        "    batch_passages = passages['passage'].iloc[i:i+batch_size].tolist()\n",
        "    batch_embeddings = embedding_model.encode(batch_passages, show_progress_bar=False)\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
      ],
      "metadata": {
        "id": "WWiDL2_nV1sn"
      },
      "id": "WWiDL2_nV1sn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Schema\n",
        "id_ = FieldSchema(\n",
        "    name=\"id\",\n",
        "    dtype=DataType.INT64,\n",
        "    is_primary=True,\n",
        "    auto_id=False\n",
        ")\n",
        "\n",
        "passage = FieldSchema(\n",
        "    name=\"passage\",\n",
        "    dtype=DataType.VARCHAR,\n",
        "    max_length=1000\n",
        ")\n",
        "\n",
        "embedding = FieldSchema(\n",
        "    name=\"embedding\",\n",
        "    dtype=DataType.FLOAT_VECTOR,\n",
        "    dim=config['embedding_dim']\n",
        ")"
      ],
      "metadata": {
        "id": "IrxhowX0ULSd"
      },
      "id": "IrxhowX0ULSd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = CollectionSchema(\n",
        "    fields=[id_, passage, embedding],\n",
        "    description=\"RAG Mini Wikipedia Collection\"\n",
        ")\n",
        "\n",
        "# Create the client\n",
        "client = MilvusClient(\"rag_wikipedia_mini.db\")"
      ],
      "metadata": {
        "id": "L5tg1Mq7UPeC"
      },
      "id": "L5tg1Mq7UPeC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Drop the collection if it already exists\n",
        "    client.drop_collection(config['rag_collection_name'])\n",
        "except:\n",
        "    pass\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=config['rag_collection_name'],\n",
        "    schema=schema\n",
        ")\n",
        "\n",
        "print(\"Collection created successfully\")"
      ],
      "metadata": {
        "id": "mRHwSYrVI_lm"
      },
      "id": "mRHwSYrVI_lm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passages_reset = passages.reset_index(drop=True)\n",
        "rag_data = []\n",
        "for idx, row in passages_reset.iterrows():\n",
        "    rag_data.append({\n",
        "        \"id\": int(idx),\n",
        "        \"passage\": row['passage'],\n",
        "        \"embedding\": embeddings[idx].tolist()\n",
        "    })\n",
        "\n",
        "print(f\"Prepared {len(rag_data)} records for insertion\")"
      ],
      "metadata": {
        "id": "8zYpfNTFJDPV"
      },
      "id": "8zYpfNTFJDPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to insert the data to your DB\n",
        "print(\"Inserting data into Milvus...\")\n",
        "res = client.insert(collection_name=\"rag_mini\", data=rag_data)\n",
        "print(f\"Insert result: {res}\")"
      ],
      "metadata": {
        "id": "gLCBce1PUlew"
      },
      "id": "gLCBce1PUlew",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Do a Sanity Check on your database\n",
        "\n",
        "**Do not delete the below line during your submission**"
      ],
      "metadata": {
        "id": "dpNESSsFw4EO"
      },
      "id": "dpNESSsFw4EO"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Entity count:\", client.get_collection_stats(\"rag_mini\")[\"row_count\"])\n",
        "print(\"Collection schema:\", client.describe_collection(\"rag_mini\"))"
      ],
      "metadata": {
        "id": "q-M4fpawUnkX"
      },
      "id": "q-M4fpawUnkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\")\n",
        "\n",
        "print(f\"Queries dataset shape: {queries.shape}\")\n",
        "print(f\"Queries columns: {queries.columns.tolist()}\")\n"
      ],
      "metadata": {
        "id": "IMu2d1-iUp8H"
      },
      "id": "IMu2d1-iUp8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the queries dataset\n",
        "queries = queries.dropna(subset=['question', 'answer'])\n",
        "queries = queries[queries['question'].str.strip() != '']\n",
        "queries = queries[queries['answer'].str.strip() != '']\n",
        "\n",
        "print(f\"Cleaned queries shape: {queries.shape}\")\n",
        "print(\"\\nSample queries:\")\n",
        "print(queries.head())"
      ],
      "metadata": {
        "id": "4VTBdU5lUrlt"
      },
      "id": "4VTBdU5lUrlt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Question\n",
        "query = queries['question'].iloc[0]\n",
        "print(f\"Test query: {query}\")\n",
        "\n",
        "query_embedding = embedding_model.encode([query])\n",
        "print(f\"Query embedding shape: {query_embedding.shape}\")"
      ],
      "metadata": {
        "id": "Sw56xE47UuM9"
      },
      "id": "Sw56xE47UuM9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Index on the embedding column\n",
        "index_params = MilvusClient.prepare_index_params()\n",
        "\n",
        "# Add an index on the embedding field\n",
        "index_params.add_index(\n",
        "    field_name=\"embedding\",\n",
        "    index_type=\"FLAT\",\n",
        "    metric_type=\"COSINE\"\n",
        ")\n",
        "\n",
        "# Create the index\n",
        "try:\n",
        "    client.create_index(\n",
        "        collection_name=\"rag_mini\",\n",
        "        index_params=index_params\n",
        "    )\n",
        "    print(\"Index created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Index creation result: {e}\")"
      ],
      "metadata": {
        "id": "r8W7rlGOUuKg"
      },
      "id": "r8W7rlGOUuKg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load collection into memory\n",
        "client.load_collection(\"rag_mini\")\n",
        "print(\"Collection loaded into memory\")\n",
        "\n",
        "# Search the db\n",
        "search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "\n",
        "output_ = client.search(\n",
        "    collection_name=\"rag_mini\",\n",
        "    data=query_embedding.tolist(),\n",
        "    anns_field=\"embedding\",\n",
        "    search_params=search_params,\n",
        "    limit=10,\n",
        "    output_fields=[\"passage\"]\n",
        ")\n",
        "\n",
        "print(\"Search results:\")\n",
        "for i, result in enumerate(output_[0]):\n",
        "    print(f\"Rank {i+1}: Score: {result['distance']:.4f}\")\n",
        "    print(f\"Passage: {result['entity']['passage'][:200]}...\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "LVxCBpFVUuHB"
      },
      "id": "LVxCBpFVUuHB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLM\n",
        "model_name = config['llm']\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on device: {device}\")\n"
      ],
      "metadata": {
        "id": "m_6ESTIfUuBU"
      },
      "id": "m_6ESTIfUuBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rag_response(question, top_k=5, max_context_chars=800):\n",
        "    # Get query embedding\n",
        "    query_embedding = embedding_model.encode([question])\n",
        "\n",
        "    # Search database\n",
        "    search_results = client.search(\n",
        "        collection_name=\"rag_mini\",\n",
        "        data=query_embedding.tolist(),\n",
        "        anns_field=\"embedding\",\n",
        "        search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
        "        limit=top_k,\n",
        "        output_fields=[\"passage\"]\n",
        "    )\n",
        "\n",
        "    # Enhancement 1: Confidence Scoring\n",
        "    scores = [result['distance'] for result in search_results[0]]\n",
        "    confidence = np.mean(scores)\n",
        "\n",
        "    # Adaptive context selection based on confidence\n",
        "    if confidence > 0.75:\n",
        "        selected_contexts = search_results[0][:2]\n",
        "        certainty_note = \"Based on highly relevant information\"\n",
        "    else:\n",
        "        selected_contexts = search_results[0][:4]\n",
        "        certainty_note = \"Based on available information with moderate confidence\"\n",
        "\n",
        "    # Enhancement 2: Context Window Optimization\n",
        "    def optimize_context_window(contexts, max_chars):\n",
        "        combined = \" \".join([ctx['entity']['passage'] for ctx in contexts])\n",
        "\n",
        "        if len(combined) <= max_chars:\n",
        "            return combined\n",
        "\n",
        "        sentences = combined.split('. ')\n",
        "        optimized = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(optimized + sentence + \". \") <= max_chars:\n",
        "                optimized += sentence + \". \"\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return optimized.strip()\n",
        "\n",
        "    # Apply context window optimization\n",
        "    optimized_context = optimize_context_window(selected_contexts, max_context_chars)\n",
        "\n",
        "    # Generate response with enhanced prompting\n",
        "    system_prompt = f\"\"\"You are a extremely knowledgeable and helpful assistant. {certainty_note}, answer the question using the provided context. Be extremely concise, only respond with the answer to the question, with no other commentary or information required. If the context doesn't fully answer the question, acknowledge this limitation and say 'I don't know'.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "    Context: {optimized_context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    # Generate answer\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=400, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=100,\n",
        "            num_beams=4,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    context_list = [ctx['entity']['passage'] for ctx in selected_contexts]\n",
        "    return generated_answer, context_list\n",
        "\n",
        "strategies = {\n",
        "    \"top1\": 1,\n",
        "    \"top3\": 3,\n",
        "    \"top5\": 5\n",
        "}\n",
        "\n",
        "results = {}"
      ],
      "metadata": {
        "id": "CtHF0V9TZM61"
      },
      "id": "CtHF0V9TZM61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate responses for different strategies\n",
        "\n",
        "for strategy_name, top_k in strategies.items():\n",
        "    print(f\"\\nGenerating responses with {strategy_name} strategy...\")\n",
        "\n",
        "    strategy_results = {\n",
        "        'questions': [],\n",
        "        'generated_answers': [],\n",
        "        'ground_truth_answers': [],\n",
        "        'contexts': []\n",
        "    }\n",
        "\n",
        "    for idx, row in tqdm(queries.iterrows(), total=len(queries), desc=f\"Processing {strategy_name}\"):\n",
        "        question = row['question']\n",
        "        ground_truth = row['answer']\n",
        "\n",
        "        generated_answer, contexts = get_rag_response(question, top_k=top_k)\n",
        "\n",
        "        strategy_results['questions'].append(question)\n",
        "        strategy_results['generated_answers'].append(generated_answer)\n",
        "        strategy_results['ground_truth_answers'].append(ground_truth)\n",
        "        strategy_results['contexts'].append(contexts)\n",
        "\n",
        "    results[strategy_name] = strategy_results\n",
        "\n",
        "print(\"\\nResponse generation completed!\")\n"
      ],
      "metadata": {
        "id": "pnXmkrboUt3j"
      },
      "id": "pnXmkrboUt3j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['top3']['generated_answers'][:15]"
      ],
      "metadata": {
        "id": "cu9nJ0rG2yfs"
      },
      "id": "cu9nJ0rG2yfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s):\n",
        "    # Normalize answer for comparison\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    # Calculate exact match score\n",
        "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def f1_score_qa(prediction, ground_truth):\n",
        "    # Calculate F1 score\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "print(\"QA METRICS EVALUATION\")\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for strategy_name, strategy_data in results.items():\n",
        "    print(f\"\\nEvaluating {strategy_name} strategy:\")\n",
        "\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for pred, truth in zip(strategy_data['generated_answers'], strategy_data['ground_truth_answers']):\n",
        "        em_score = exact_match_score(pred, truth)\n",
        "        f1_score = f1_score_qa(pred, truth)\n",
        "\n",
        "        em_scores.append(em_score)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "    avg_em = np.mean(em_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    evaluation_results[strategy_name] = {\n",
        "        'exact_match': avg_em,\n",
        "        'f1_score': avg_f1,\n",
        "        'em_scores': em_scores,\n",
        "        'f1_scores': f1_scores\n",
        "    }\n",
        "\n",
        "    print(f\"Exact Match: {avg_em:.4f}\")\n",
        "    print(f\"F1 Score: {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "Y0Zr2YQOU7uM"
      },
      "id": "Y0Zr2YQOU7uM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results comparison\n",
        "print(\"STRATEGY COMPARISON\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Strategy': list(evaluation_results.keys()),\n",
        "    'Exact Match': [evaluation_results[k]['exact_match'] for k in evaluation_results.keys()],\n",
        "    'F1 Score': [evaluation_results[k]['f1_score'] for k in evaluation_results.keys()]\n",
        "})\n",
        "\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "c8m1eKuTU7ri"
      },
      "id": "c8m1eKuTU7ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best strategy identification\n",
        "best_f1_strategy = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Strategy']\n",
        "best_em_strategy = comparison_df.loc[comparison_df['Exact Match'].idxmax(), 'Strategy']\n",
        "\n",
        "print(f\"Best strategy by F1 Score: {best_f1_strategy}\")\n",
        "print(f\"Best strategy by Exact Match: {best_em_strategy}\")\n"
      ],
      "metadata": {
        "id": "EMRv1uZ8U7oT"
      },
      "id": "EMRv1uZ8U7oT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_results = results[best_f1_strategy]"
      ],
      "metadata": {
        "id": "2mrdRSy1YDYk"
      },
      "id": "2mrdRSy1YDYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI API for RAGAs\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "B_uYvzFFzgYw"
      },
      "id": "B_uYvzFFzgYw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Evaluation using RAGAs"
      ],
      "metadata": {
        "id": "ObAabReRyUsy"
      },
      "id": "ObAabReRyUsy"
    },
    {
      "cell_type": "code",
      "source": [
        "ragas_data = {\n",
        "    \"question\": best_results['questions'][:50],\n",
        "    \"answer\": best_results['generated_answers'][:50],\n",
        "    \"contexts\": [[context] for context in [ctx[0] for ctx in best_results['contexts']]][:50],\n",
        "    \"ground_truth\": best_results['ground_truth_answers'][:50]\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(ragas_data)\n",
        "\n",
        "print(f\"Dataset prepared for RAGAs evaluation with {len(dataset)} samples\")\n"
      ],
      "metadata": {
        "id": "ZIPbsUkk0AJQ"
      },
      "id": "ZIPbsUkk0AJQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate(\n",
        "        dataset,\n",
        "        metrics=[\n",
        "            faithfulness,\n",
        "            answer_relevancy,\n",
        "            context_precision,\n",
        "            context_recall\n",
        "        ],\n",
        "        raise_exceptions=False\n",
        "        )"
      ],
      "metadata": {
        "id": "QluS5WTB0pQt"
      },
      "id": "QluS5WTB0pQt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_data = {\n",
        "    'metadata': {\n",
        "        'date': datetime.now().isoformat(),\n",
        "        'dataset': 'RAG Mini Wikipedia',\n",
        "        'total_queries': len(queries),\n",
        "        'embedding_model': 'all-mpnet-base-v2',\n",
        "        'llm_model': 'google/flan-t5-base',\n",
        "        'embedding_dim': config['embedding_dim'],\n",
        "        'prompting_style': config['prompting_style'],\n",
        "    },\n",
        "    'strategy_comparison': comparison_df.to_dict('records'),\n",
        "    'detailed_results': {}\n",
        "}\n",
        "\n",
        "# Add results for each strategy\n",
        "for strategy_name, strategy_data in evaluation_results.items():\n",
        "    results_data['detailed_results'][strategy_name] = {\n",
        "        'exact_match': float(strategy_data['exact_match']),\n",
        "        'f1_score': float(strategy_data['f1_score']),\n",
        "        'num_samples': len(strategy_data['em_scores']),\n",
        "        'statistics': {\n",
        "            'em_std': float(np.std(strategy_data['em_scores'])),\n",
        "            'f1_std': float(np.std(strategy_data['f1_scores'])),\n",
        "            'em_min': float(np.min(strategy_data['em_scores'])),\n",
        "            'em_max': float(np.max(strategy_data['em_scores'])),\n",
        "            'f1_min': float(np.min(strategy_data['f1_scores'])),\n",
        "            'f1_max': float(np.max(strategy_data['f1_scores']))\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Add best performing strategy\n",
        "best_f1_strategy = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Strategy']\n",
        "best_em_strategy = comparison_df.loc[comparison_df['Exact Match'].idxmax(), 'Strategy']\n",
        "\n",
        "results_data['best_strategies'] = {\n",
        "    'best_f1': best_f1_strategy,\n",
        "    'best_em': best_em_strategy,\n",
        "    'best_f1_score': float(comparison_df['F1 Score'].max()),\n",
        "    'best_em_score': float(comparison_df['Exact Match'].max())\n",
        "}\n",
        "\n",
        "ragas_df = result.to_pandas()\n",
        "results_data['ragas'] = {\n",
        "    'faithfulness': float(ragas_df['faithfulness'].mean()),\n",
        "    'answer_relevancy': float(ragas_df['answer_relevancy'].mean()),\n",
        "    'context_precision': float(ragas_df['context_precision'].mean()),\n",
        "    'context_recall': float(ragas_df['context_recall'].mean())\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_filename = f'results_advanced_{config['embedding_dim']}_{config['prompting_style']}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(results_data, indent=2, fp=f)\n",
        "\n",
        "print(f\"Results saved to {output_filename}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nResults Summary:\")\n",
        "print(comparison_df)\n",
        "print(f\"\\nBest F1 Strategy: {best_f1_strategy} ({results_data['best_strategies']['best_f1_score']:.4f})\")\n",
        "print(f\"Best EM Strategy: {best_em_strategy} ({results_data['best_strategies']['best_em_score']:.4f})\")\n",
        "print(f\"\\nRAGAS Results:\")\n",
        "print(f\"Faithfulness: {results_data['ragas']['faithfulness']:.4f}\")\n",
        "print(f\"Answer Relevancy: {results_data['ragas']['answer_relevancy']:.4f}\")\n",
        "print(f\"Context Precision: {results_data['ragas']['context_precision']:.4f}\")\n",
        "print(f\"Context Recall: {results_data['ragas']['context_recall']:.4f}\")"
      ],
      "metadata": {
        "id": "5eP4WKDh-2Yz"
      },
      "id": "5eP4WKDh-2Yz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwyvqOxUDQA8"
      },
      "id": "QwyvqOxUDQA8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}